{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# tokenization\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# classification modelling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required datasets to train NLTK models\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e62a9",
   "metadata": {},
   "source": [
    "### 1.1 Text data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e75130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text data\n",
    "dat = pd.read_csv(\"AA_movie_train_data.csv\")\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0382cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a81454",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[\"Genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbef1b7",
   "metadata": {},
   "source": [
    "### 1.2 Cleanse text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^A-Za-z]', \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88624d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(filepath: str) -> frozenset[str]:\n",
    "    with open(filepath, 'r') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text cleaning on descriptions column\n",
    "descriptions = dat[\"Description\"].dropna().astype(str)\n",
    "descriptions_all = descriptions.apply(pre_process)\n",
    "tokenized_desc = [word_tokenize(sentence) for sentence in descriptions_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_desc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "stopwords = list(get_stopwords(\"stopwords.txt\"))\n",
    "\n",
    "filtered_desc = [\n",
    "    [word for word in sentence if word not in stopwords]\n",
    "    for sentence in tokenized_desc\n",
    "]\n",
    "\n",
    "print(dat[\"Description\"][0])\n",
    "print(filtered_desc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8df595",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Apply lemmatization with POS tagging\n",
    "lemmatized_desc = [\n",
    "    [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in sentence]\n",
    "    for sentence in filtered_desc\n",
    "]\n",
    "\n",
    "print(lemmatized_desc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86741f16",
   "metadata": {},
   "source": [
    "### 1.3 Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert keywords back into str for bow\n",
    "cleaned_texts = [\" \".join(tokens) for tokens in filtered_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.8, max_features=5000)\n",
    "bow_matrix = vectorizer.fit_transform(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting word frequencies\n",
    "sum_words = bow_matrix.sum(axis=0)\n",
    "\n",
    "bow_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "bow_freq = sorted(bow_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Highest freq words: \")\n",
    "bow_freq[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a12cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05618ff3",
   "metadata": {},
   "source": [
    "### 1.4 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcf7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d25494",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_tfidf = np.argsort(tfidf_transformer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_tfidf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_transformer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tfidf_matrix.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adc1e1",
   "metadata": {},
   "source": [
    "### 2.1 Extracting keywords from TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "doc = descriptions_all[idx]\n",
    "print(doc)\n",
    "\n",
    "tf_idf_vector = tfidf_matrix[idx]\n",
    "print(tf_idf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(zip(tf_idf_vector.tocoo().col, tf_idf_vector.tocoo().data),columns=['feature_number','tf_idf'])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sort_values('tf_idf', ascending = False, inplace = True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only topn items from vector\n",
    "\n",
    "topn = 10   \n",
    "topn_items = temp[:topn]\n",
    "\n",
    "tf_idf = []\n",
    "word = []\n",
    "\n",
    "for index, row in topn_items.iterrows():\n",
    "    fname = feature_names[int(row['feature_number'])]\n",
    "    word.append(fname)\n",
    "    tf_idf.append(round(row['tf_idf'], 3))    \n",
    "\n",
    "print(doc, '\\n')\n",
    "\n",
    "result = dict(zip(word, tf_idf))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords for all reviews\n",
    "\n",
    "topn = 10\n",
    "first_results = []\n",
    "\n",
    "for idx, doc in descriptions_all.items():\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = tfidf_matrix[idx]\n",
    "    \n",
    "    temp = pd.DataFrame(zip(tf_idf_vector.tocoo().col, tf_idf_vector.tocoo().data), columns=['feature_number','tf_idf'])\n",
    "    temp.sort_values('tf_idf', ascending = False, inplace = True)\n",
    "    \n",
    "    #use only topn items from vector\n",
    "     \n",
    "    topn_items = temp[:topn]\n",
    "\n",
    "    tf_idf = []\n",
    "    word = []\n",
    "\n",
    "    for index, row in topn_items.iterrows():\n",
    "        #print(int(row['feature_number']))\n",
    "        fname = feature_names[int(row['feature_number'])]\n",
    "        word.append(fname)\n",
    "        tf_idf.append(round(row['tf_idf'], 3))\n",
    "\n",
    "    result = dict(zip(word, tf_idf))\n",
    "    first_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d96b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[\"cleaned_desc\"] = descriptions_all\n",
    "dat[\"keywords\"] = first_results\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_csv(\"descriptions_export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export lemmatized_desc to csv for ARM\n",
    "\n",
    "lemmatized_df = pd.DataFrame(lemmatized_desc)\n",
    "lemmatized_df.to_csv(\"lemmatized_desc.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8071c31",
   "metadata": {},
   "source": [
    "### 2.2 Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2f5b7",
   "metadata": {},
   "source": [
    "### Generating association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = pd.read_csv(\"lemmatized_desc.csv\", header=None)\n",
    "desc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cba903",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list=pd.Series([])\n",
    "for col in desc_data:\n",
    "    full_list = full_list._append(desc_data[col].dropna())\n",
    "\n",
    "print(full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b916ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "wordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 121).generate(str(full_list))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Most Popular Items',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the frequency of most popular items \n",
    "plt.figure(figsize=(18,7))\n",
    "full_list.value_counts().head(50).plot.bar()\n",
    "plt.title('frequency of most popular items', fontsize = 20)\n",
    "plt.xticks(rotation = 90 )\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_list.value_counts().head(50).to_frame()\n",
    "y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc16a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making each customers shopping items an identical list\n",
    "trans = []\n",
    "for i in range(0, 5000):\n",
    "    trans.append([str(desc_data.values[i,j]) for j in range(0, 20)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "\n",
    "# checking the shape of the array\n",
    "print(trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms the input dataset (a Python list of lists) into a one-hot encoded NumPy boolean array\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "# getting the shape of the data\n",
    "data_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d045c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "# checking the shape\n",
    "data_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796315b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let us return the items and itemsets with at least 1% support:\n",
    "frequent_itemsets = apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[['support']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18971bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "rules_l.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_l[['lift']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b470202",
   "metadata": {},
   "source": [
    "### BOW MODELLING.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution\n",
    "print(\"Genre distribution:\")\n",
    "print(dat[\"Genre\"].value_counts())\n",
    "print(f\"\\nTotal samples: {len(dat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c291aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing genre or description\n",
    "\n",
    "model_data = dat.dropna(subset=['Description', 'Genre']).copy()\n",
    "print(f\"Data after removing NaN: {len(model_data)} samples\")\n",
    "\n",
    "valid_indices = model_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebbc21",
   "metadata": {},
   "source": [
    "#### BOW models first\n",
    "- LR\n",
    "- RFC\n",
    "- SVC\n",
    "- GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow_matrix\n",
    "y = model_data[\"Genre\"].values\n",
    "\n",
    "X_dense = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e93dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT FORGET TO SCALE VALUES\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a16419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode genre labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    train_size=(.60), \n",
    "    random_state=42, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4942f",
   "metadata": {},
   "source": [
    "#### 1st round of training BOW models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4275f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model hyperparams\n",
    "\n",
    "first_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "}\n",
    "\n",
    "first_results = {}\n",
    "\n",
    "for name, model in first_models.items():\n",
    "    print(f\"Training {model}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "    first_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"cv_mean\": cv_score.mean(),\n",
    "        \"cv_std\": cv_score.std(),\n",
    "        \"predictions\": y_pred\n",
    "    }\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Model': first_results.keys(),\n",
    "    'Test Accuracy': [first_results[model]['accuracy'] for model in first_results.keys()],\n",
    "    'CV Mean': [first_results[model]['cv_mean'] for model in first_results.keys()],\n",
    "    'CV Std': [first_results[model]['cv_std'] for model in first_results.keys()]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "print(\"\\nModel Performance Summary:\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = first_results[best_model_name]['model']\n",
    "best_predictions = first_results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {first_results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\") \n",
    "\n",
    "report = classification_report(\n",
    "    y_test, \n",
    "    best_predictions,\n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True\n",
    ")\n",
    "print(classification_report(y_test, best_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004334a",
   "metadata": {},
   "source": [
    "#### GridSearchCV for BOW LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since LR gave back best results, we tune for LR\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# need to specify valid combinations\n",
    "# lbfgs doesnt support L1 penalty, might raise internal error\n",
    "param_grid = [\n",
    "    { 'penalty': ['l2'], 'solver': ['lbfgs'], 'C': [0.1, 1, 10] },\n",
    "    { 'penalty': ['l1', 'l2'], 'solver': ['saga'], 'C': [0.1, 1, 10] }\n",
    "]\n",
    "\n",
    "# custom f1 scorer for gridsearch\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5106367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    tuning_model, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1   # use all processors\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff524f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
